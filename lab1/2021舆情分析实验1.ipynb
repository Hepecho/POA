{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "舆情分析实验（1）文本主题分类\n",
    "==================================\n",
    "\n",
    "实验目的\n",
    "---------------------\n",
    "   - 熟悉简单的文本处理流程和神经网络文本分类模型\n",
    "   \n",
    "     文本分类是将文本按照一定的标准进行分类，比如新闻可以按照内容主题分为体育、科学等等。分类的“规则”可以由人确定，也可以用算法从有标签数据中自动归纳。人工制定分类规则可能并不容易，比如要区分体育和科学新闻，以有没有出现体育项目和运动员姓名来分类是可行的，但制定查找表并不容易，并且不能排除主要讲科学的新闻中出现这些词。自动算法帮助我们从文本中提取特征进行分类，并具有良好的性能和适应性。\n",
    "       - 利用简单的神经网络对文本主题进行分类\n",
    "       \n",
    "       输入：一段文本\n",
    "       \n",
    "       输出：预先定义好的主题类别，如：体育、科学等\n",
    "       \n",
    "       \n",
    "   - 初步了解深度学习框架pytorch的使用\n",
    "   \n",
    "     pytorch是一个开源深度学习框架，是一个基于Python的可续计算包，提供两个高级功能：1、具有强大的GPU加速的张量计算（如NumPy）。2、包含自动求导系统的深度神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验环境\n",
    "---------------------\n",
    "   - Anaconda（当前python3.8）\n",
    "   \n",
    "   https://www.anaconda.com/products/individual-d\n",
    "   \n",
    "   - pytorch（当前torch 1.9.1） pip install torch\n",
    "      - pytorch是一个开源的深度学习框架，能够利用GPU加速计算\n",
    "   \n",
    "   - torchtext（当前torchtext 0.10.1）\n",
    "   pip install torchtext\n",
    "   \n",
    "   - cuda（英伟达显卡） GPU加速必备\n",
    "   参考教程\n",
    "   \n",
    "   https://blog.csdn.net/weixin_43848614/article/details/117221384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验数据\n",
    "---------------------\n",
    "\n",
    "新闻数据集AG_NEWS\n",
    "\n",
    "AG_NEWS数据集由4大类主题\n",
    "\n",
    "(\"World\",\"Sports\",\"Business\",\"Sci/Tech\")\n",
    "\n",
    "的新闻的标题和描述字段组合而成，每类包含 30,000 个训练和 1,900 个测试样本。\n",
    "\n",
    "如不能自动下载请访问以下链接\n",
    "\n",
    "\n",
    "train:`https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv`\n",
    "\n",
    "test:`https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验步骤\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "访问原始数据集\n",
    "---------------------\n",
    "\n",
    "torchtext 库提供了一些原始数据集迭代器，它们给出原始文本字符串。 例如，“AG_NEWS”数据集迭代器将原始数据以标签和文本的元组给出。\n",
    "\n",
    "(\"World\",\"Sports\",\"Business\",\"Sci/Tech\")对应于标签(\"1\",\"2\",\"3\",\"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class AG_NEWS(Dataset):\n",
    "    def __init__(self, data_name):\n",
    "        self.path = f'{data_name}.csv'\n",
    "        self.process_data()\n",
    "\n",
    "    def process_data(self):\n",
    "        df = pd.read_csv(self.path, header=None)\n",
    "        self.label_list = df.iloc[:, 0].values.tolist()\n",
    "        self.data_list = df.iloc[:, 1].values.tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.label_list[idx], self.data_list[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "train_iter = AG_NEWS('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::\n",
    "\n",
    "    next(train_iter)\n",
    "    >>> (3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - \n",
    "    Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green \n",
    "    again.\")\n",
    "\n",
    "    next(train_iter)\n",
    "    >>> (3, 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private \n",
    "    investment firm Carlyle Group,\\\\which has a reputation for making well-timed \n",
    "    and occasionally\\\\controversial plays in the defense industry, has quietly \n",
    "    placed\\\\its bets on another part of the market.')\n",
    "\n",
    "    next(train_iter)\n",
    "    >>> (3, \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring \n",
    "    crude prices plus worries\\\\about the economy and the outlook for earnings are \n",
    "    expected to\\\\hang over the stock market next week during the depth of \n",
    "    the\\\\summer doldrums.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备数据处理管道\n",
    "---------------------------------\n",
    "\n",
    "第一步是使用原始训练数据集构建词汇表。`get_tokenizer`函数对文本进行分词。这里我们使用内置函数`build_vocab_from_iterator` 接受迭代器,它能自动构建词汇表，并把传入的文本字符串转成代表token的整数序列。还可以将任何特殊符号自主添加到词汇表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "# 分词\n",
    "train_iter = iter(AG_NEWS('train'))\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# 用<unk>填充那些不在词表中的词。\n",
    "# 词表很难包括所有可能出现的词，这称为out of vocabulary问题（OOV）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab将token列表转换为整数。\n",
    "\n",
    "::\n",
    "\n",
    "     vocab(['here', 'is', 'an', 'example'])\n",
    "     >>> [475, 21, 30, 5286]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[475, 21, 30, 5297]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本管道根据词汇表中定义的token查找表将文本字符串转换为整数列表。标签管道将标签转换为整数。 例如，\n",
    "\n",
    "::\n",
    "\n",
    "     text_pipeline('here is the an example')\n",
    "     >>> [475, 21, 2, 30, 5286]\n",
    "     label_pipeline('3')\n",
    "     >>> 2\n",
    "\n",
    "注意'3'是字符串格式，转换成整数2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据批处理函数和加载器\n",
    "--------------------------------\n",
    "\n",
    "`torch.utils.data.DataLoader `\n",
    "它与实现\"getitem()\"和\"len()\"的数据集类一起使用，并表示从索引/键到数据样本的映射。\n",
    "\n",
    "在发送到模型之前，``collat​​e_fn`` 函数处理从``DataLoader`` 读取的一批样本。 ``collat​​e_fn`` 的输入是在``DataLoader`` 中具有大小batch的一批数据，``collat​​e_fn`` 根据之前声明的数据处理函数对其进行处理。\n",
    "\n",
    "在这个例子中，原始数据批量输入中的文本条目被打包到一个列表中，并连接为 nn.EmbeddingBag 输入的单个张量。offsets是偏移量的张量，用于表示文本张量中单个序列的开始索引。label是保存单个文本条目标签的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 检测是否有安装了cuda的显卡可用\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "        # 所谓偏移量可以理解为该文本的长度\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)    \n",
    "\n",
    "train_iter = AG_NEWS('train')\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "# shuffle=True将打乱数据集，batch_size使得加载器每次读取该数量的条目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型\n",
    "----------------\n",
    "\n",
    "该模型由 `nn.EmbeddingBag层和用于分类目的的线性层组成。\n",
    "\n",
    "尽管此处的每条文本具有不同的长度，但 nn.EmbeddingBag 模块在此处不需要填充，因为文本长度保存在偏移量中。\n",
    "\n",
    "\n",
    "![jupyter](https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        self.fc = nn.Linear(embed_dim, num_class) #这个就是线性层\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "固定随机数种子\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=6):\n",
    "\n",
    "    #random.seed(seed)\n",
    "    # os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    #np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启动一个实例\n",
    "--------------------\n",
    "\n",
    "``AG_NEWS`` 数据集有四个标签，因此类的数量是四个。\n",
    "\n",
    "::\n",
    "\n",
    "    1： 世界\n",
    "    2 : 运动\n",
    "    3 : 商务\n",
    "    4 : 科学/技术\n",
    "\n",
    "我们构建了一个嵌入维度为 64 的模型。类的数量等于标签的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(AG_NEWS('train'))\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义用于训练模型和评估结果的函数。\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拆分数据集并运行模型\n",
    "---------------------\n",
    "\n",
    "由于原始``AG_NEWS`` 没有用来验证的数据集，我们拆分\n",
    "将训练数据集转换为训练集/验证集，分割比为 0.95（训练集）和\n",
    "0.05（验证集）。我们使用\n",
    "`torch.utils.data.dataset.random_split`\n",
    "\n",
    "`CrossEntropyLoss`\n",
    "交叉熵损失函数，分类问题常用。\n",
    "\n",
    "`SGD`\n",
    "随机梯度下降法作为优化器。最初的\n",
    "学习率设置为 5.0。\n",
    "\n",
    "`StepLR`\n",
    "用来调整学习率。\n",
    "\n",
    "CPU参考运行时间：7分钟\n",
    "\n",
    "GPU参考运行时间：5分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.687\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.855\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.877\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 23.00s | valid accuracy    0.884 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.900\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.899\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.903\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 23.40s | valid accuracy    0.891 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.915\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.913\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.915\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 23.23s | valid accuracy    0.897 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.926\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.924\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.922\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 23.24s | valid accuracy    0.908 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.930\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.930\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.929\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 23.47s | valid accuracy    0.906 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.942\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.942\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 23.22s | valid accuracy    0.912 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.946\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.944\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.944\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 23.18s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.945\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.947\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.945\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 23.47s | valid accuracy    0.913 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.947\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.946\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.945\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 23.52s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.944\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.946\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.947\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 23.25s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "# from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters超参数\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate学习率\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "#定义损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#定义优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "# train_iter, test_iter = AG_NEWS()\n",
    "# train_iter, test_iter是迭代器的形式，to_map_style_dataset将其\n",
    "# 转成DataLoader能读取的格式，参考torch.utils.data.Dataset类\n",
    "# train_dataset = to_map_style_dataset(train_iter)\n",
    "# test_dataset = to_map_style_dataset(test_iter)\n",
    "train_dataset = AG_NEWS('train')\n",
    "test_dataset = AG_NEWS('test')\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用测试数据集评估模型\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查测试数据集的结果…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.908\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一条新闻\n",
    "---------------------\n",
    "\n",
    "使用该模型并测试一条高尔夫新闻（体育）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text.to(device), torch.tensor([0]).to(device))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用LSTM模型\n",
    "---------------------\n",
    "\n",
    "长短期记忆网络——通常被称为 LSTM，是一种特殊的 RNN，能够学习长期依赖性。由 Hochreiter 和 Schmidhuber（1997）提出。\n",
    "\n",
    "![jupyter](https://n.sinaimg.cn/spider202044/731/w1040h491/20200404/0bc7-irtymmw0458671.png)\n",
    "\n",
    "CPU参考运行时间：30分钟\n",
    "\n",
    "GPU参考运行时间：8分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, bidirectional=False)\n",
    "        # num_layers定义了LSTM网络的层数，bidirectional表示是单向LSTM还是双向LSTM（BiLSTM）\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets).unsqueeze(1)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(embedded)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(AG_NEWS('train'))\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "hidden_size = 100\n",
    "num_layers = 1\n",
    "LSTM_model = LSTM(vocab_size, emsize, num_class, hidden_size, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    LSTM_model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = LSTM_model(text, offsets).squeeze()\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(LSTM_model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    LSTM_model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = LSTM_model(text, offsets).squeeze()\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.489\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.779\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.836\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 31.15s | valid accuracy    0.828 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.870\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.880\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.885\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 31.35s | valid accuracy    0.883 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.902\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.899\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.905\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 31.27s | valid accuracy    0.888 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.915\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.913\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.915\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 31.15s | valid accuracy    0.893 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.922\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.923\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.922\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 31.27s | valid accuracy    0.896 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.934\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.929\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.929\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 31.17s | valid accuracy    0.893 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.945\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.946\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.945\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 31.09s | valid accuracy    0.906 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.946\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.947\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.949\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 31.12s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.951\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.947\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.951\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 31.07s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.951\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.949\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.950\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 31.29s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "# from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters超参数\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate学习率\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "  \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(LSTM_model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "# train_iter, test_iter = AG_NEWS()\n",
    "# train_dataset = to_map_style_dataset(train_iter)\n",
    "# test_dataset = to_map_style_dataset(test_iter)\n",
    "train_dataset = AG_NEWS('train')\n",
    "test_dataset = AG_NEWS('test')\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验要求\n",
    "-------------------\n",
    "\n",
    "   - 配置实验环境，阅读教程样例并运行，理解实验流程\n",
    "    \n",
    "   - 将教程中的单层线性模型改为双层模型，并比较至少三组超参数和两种激活函数，写入实验报告\n",
    "       - 需要了解的内容：激活函数，如nn.Tanh()等\n",
    "   \n",
    "   - 解除TextClassificationModel中一行代码的注释，再次运行模型，比较前后结果，并解释为什么，写入实验报告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
